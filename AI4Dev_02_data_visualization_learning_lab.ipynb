{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maurapintor/ai4dev/blob/main/AI4Dev_02_data_visualization_learning_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fO_scrJRCiK"
      },
      "source": [
        "# Data sampling, visualization, learning and classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQDIueXWs_0c"
      },
      "source": [
        "For this tutorial, we will be using some libraries that are created for Machine Learning and data visualization.\n",
        "\n",
        "- data analysis and machine learning: [`scikit-learn`](https://scikit-learn.org/stable/) / (a.k.a. `sklearn`)\n",
        "- visualization: [`matplotlib`](https://matplotlib.org/stable/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEgpaL55xLmn"
      },
      "source": [
        "Let's start from a simple dataset created with scikit-learn.\n",
        "\n",
        "We will start from a [random blobs dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs).\n",
        "\n",
        "Let's create the dataset and inspect its characteristics. We will create blobs of samples that are centered in specific points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEijXjoMxK2w"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "samples, labels = ...\n",
        "\n",
        "print(samples.shape, labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v3AqPmNAqXI"
      },
      "outputs": [],
      "source": [
        "print(samples[:5], labels[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW0DLqVJABDn"
      },
      "source": [
        "As you see, if we create samples with sklearn, the data has specific shapes.\n",
        "\n",
        "In machine learning applications, in general, the first index is reserved to the sample index. For this reason, we have 100 rows in our `samples` matrix. Since we created the dataset with two features, we have 2 columns in our matrix.\n",
        "\n",
        "As for the labels, you can see that the array has only two possible values. These correspond to the class of the points (this is indeed a classification problem). This means that if the sample comes from the distribution labeled as `0`, it will be likely closer to the first center specified. Remember that this is not certain, as these are Gaussian distributions (it means that is unlikely, but the point can also come from the other distribution - just with lower probability)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z83vlFRiBkhC"
      },
      "source": [
        "Let's now visualize the points. We have to define a function that extracts the samples from each of the two classes separately, and then visualize them. Since the data are 2-dimensional, we can visualize them in a scatter plot where we represent each feature in one separate axis.\n",
        "\n",
        "To plot samples, we can use the function scatter from `matplotlib.pyplot`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5wAsHc6Q_4t"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_dataset(x, y):\n",
        "    # setup color map with default colors from matplotlib\n",
        "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
        "    colors = prop_cycle.by_key()['color']\n",
        "\n",
        "    # Colormap object generated from a list of colors.\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    available_classes = ...  # takes the unique elements\n",
        "    for c in sorted(available_classes):\n",
        "        samples_from_class = ...\n",
        "        x_coords = ...\n",
        "        y_coords = ...\n",
        "        plt.scatter(x_coords, y_coords, label=str(c), color=colors[c])\n",
        "    plt.xlabel('feature x1')\n",
        "    plt.ylabel('feature x2')\n",
        "    plt.legend()\n",
        "\n",
        "plot_dataset(samples, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqQkoutpC9EV"
      },
      "source": [
        "As you can see, the distributions are centered in the specified centers. Now, our classification problem will be to classify new points that don't belong to this dataset (thus they are not labeled)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAQNubJIRPQq"
      },
      "source": [
        "## Learning and classification\n",
        "\n",
        "Now we can sample data and visualize it in two dimensions.\n",
        "\n",
        "- The goal of the next exercises is to train a simple classifier\n",
        "- We will use a simple linear classifier\n",
        "- Then we will use its learning and classification methods\n",
        "\n",
        "During the learning phase, our classifier is given a training set consisting of pairs (x, y) of\n",
        "samples along with their labels\n",
        "\n",
        "During classification, the classifiers assigns the current test sample x to one of the classes\n",
        "\n",
        "We will use the functions:\n",
        "- `fit(x,y)`, corresponding to the learning phase, and\n",
        "- `predict(x)`, corresponding to the classification phase\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04q02GHRRPgT"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "samples, labels = datasets.make_blobs(\n",
        "    n_samples = 200,  # generate 100 samples\n",
        "    n_features = 2,  # each sample has 2 features\n",
        "    centers = [[-1, -1], [1, 1]]\n",
        ")\n",
        "\n",
        "# Support Vector Classifier\n",
        "classifier = ...\n",
        "normalizer = ...\n",
        "\n",
        "# normalize the data\n",
        "samples = ...\n",
        "\n",
        "# train the classifier\n",
        "...\n",
        "\n",
        "\n",
        "# test point\n",
        "x0 = np.array([-0.95, -0.8])\n",
        "\n",
        "# transform the sample and predict\n",
        "x0 = ...\n",
        "print(x0)\n",
        "y_pred = ...\n",
        "\n",
        "plot_dataset(samples, labels)\n",
        "plt.scatter(x0[0, 0], x0[0, 1], color='red')\n",
        "plt.text(x0[0, 0] - 0.1, x0[0, 1] - 0.1, f\"y_pred = {y_pred.item()}\",\n",
        "         bbox=dict(facecolor='white'))\n",
        "\n",
        "print(\"Predicted:\", y_pred.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO_JUoFLRykL"
      },
      "source": [
        "## Visualizing the decision regions of a classifier\n",
        "\n",
        "With matplotlib, we can also visualize the decision regions of the classifier, i.e., the prediction for all points of the feature space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiypPDWDRzcX"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_regions(x, y, classifier, num_points):\n",
        "    # setup color map with default colors from matplotlib\n",
        "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
        "    colors = prop_cycle.by_key()['color']\n",
        "\n",
        "    # Colormap object generated from a list of colors.\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # get maximum and minimum value for each feature (with a bit of margin)\n",
        "    margin = 0.1\n",
        "    x1_min, x1_max = x[:, 0].min() - margin, x[:, 0].max() + margin\n",
        "    x2_min, x2_max = x[:, 1].min() - margin, x[:, 1].max() + margin\n",
        "\n",
        "    # create coordinate matrices from coordinate vectors\n",
        "    xx1, xx2 = ...\n",
        "\n",
        "    # get color value for each coordinate (as feature 1 and feature 2)\n",
        "    z = ...\n",
        "\n",
        "    # shape again as a grid\n",
        "    z = ...\n",
        "\n",
        "    # plot the colors\n",
        "    ...\n",
        "\n",
        "    # set plot limits\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # plot class samples\n",
        "    plot_dataset(x,y)\n",
        "\n",
        "    return\n",
        "\n",
        "plot_decision_regions(samples, labels, classifier, 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV-_und9NtwK"
      },
      "source": [
        "Now let's try it with more complicated datasets and more samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKUCeNT8R2u4"
      },
      "outputs": [],
      "source": [
        "samples, labels = datasets.make_blobs(\n",
        "    n_samples = 1000,  # generate 100 samples\n",
        "    n_features = 2,  # each sample has 2 features\n",
        "    centers = [[-1, -3], [3, 2], [-2, 3], [3, -4]]\n",
        ")\n",
        "\n",
        "classifier = SVC(kernel='linear')\n",
        "samples = normalizer.fit_transform(samples)\n",
        "classifier.fit(samples, labels)\n",
        "plot_decision_regions(samples, labels, classifier, num_points=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkGwXzNZR5kW"
      },
      "source": [
        "## Testing performances on unseen data\n",
        "\n",
        "To assess classifier performance, one should estimate the classification error on never-\n",
        "before-seen data\n",
        "\n",
        "- The training data should not be used to this end, as it provides an optimistic estimate of the\n",
        "real performance!\n",
        "\n",
        "Therefore, the correct procedure amounts to:\n",
        "1. Sampling a training and a testing set (from the same underlying distribution), e.g., with\n",
        "our data generation class from sklearn\n",
        "2. Fitting the classifier on training data\n",
        "3. Predicting the class labels of testing data\n",
        "4. Evaluating the fraction of wrong predictions\n",
        "\n",
        "We will slightly modify the process by using the sklearn APIs for splitting the dataset into training and testing set. Thus, we sample once all the data and then we split in two sub-datasets later. The rest of the process remains the same.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSXznATkR-Ez"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = datasets.make_blobs(\n",
        "    n_samples = 1000,  # generate 100 samples\n",
        "    n_features = 2,  # each sample has 2 features\n",
        "    centers = [[-1, -3], [3, 2], [-2, 3], [3, -4]])\n",
        "x_test, y_test = datasets.make_blobs(\n",
        "    n_samples = 1000,  # generate 100 samples\n",
        "    n_features = 2,  # each sample has 2 features\n",
        "    centers = [[-1, -3], [3, 2], [-2, 3], [3, -4]])\n",
        "\n",
        "\n",
        "classifier = SVC(kernel='linear')\n",
        "normalizer = MinMaxScaler()\n",
        "\n",
        "x_train = normalizer.fit_transform(x_train)\n",
        "x_test = ...\n",
        "\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "predictions = ...\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    return ...\n",
        "\n",
        "accuracy = compute_accuracy(y_test, predictions)\n",
        "\n",
        "plot_decision_regions(x_test, y_test, classifier, num_points=200)\n",
        "plt.text(0.02, 0.02, \"Test. Acc.: {:.1%}\".format(accuracy),\n",
        "        bbox=dict(facecolor='white'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzS-1Uw3R-qD"
      },
      "source": [
        "Lessons learned\n",
        "- Visualizing data and decision regions\n",
        "- Using a simple classifier (using a sklearn class)\n",
        "- Basic estimation of classifier performance on unseen data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notebook with complete code: \n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/battistabiggio/ai4dev/blob/main/notebooks/AI4Dev_02_data_visualization_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOfAp72G6E+knsk1IIpa379",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
