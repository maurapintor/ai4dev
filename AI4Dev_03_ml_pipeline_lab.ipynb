{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maurapintor/ai4dev/blob/main/AI4Dev_03_ml_pipeline_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3UGUuMj2Cfl"
      },
      "source": [
        "# ML model training pipeline\n",
        "\n",
        "To create good ML models, we need to design a full process in which we can inspect the behavior of the models under different settings.\n",
        "\n",
        "Some models can be tuned to have specific behaviors under the effect of specific settings named **hyperparameters**. These are model configurations that change the shape of the decision function and other aspects of the learning process.\n",
        "\n",
        "To pick the best hyperparameters, we have to test different instances of models and rank them according to the performance scores obtained.\n",
        "\n",
        "Testing the model on the training dataset causes the model to perform very well on the training data, but we risk failure in predicting unseen data.\n",
        "This issue is called **overfitting** and its effect prevents the model from **generalizing**.\n",
        "\n",
        "To avoid overfitting, it is common practice to:\n",
        "* take a part of the available data for training\n",
        "* take the rest for testing (unseen in training)\n",
        "\n",
        "Now, if we want also to pick the hyperparameters that generalize well, we have to train the model with a set and evaluate on a separate set.\n",
        "This is why we introduce a third subset called the validation set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wol3sK3oKlwK"
      },
      "source": [
        "The order of steps and the role of each set is the following:\n",
        "\n",
        "1. Take the available data and split in design set and testing set. The testing set should be used only at the end (i.e., even after the selection of the hyperparameters). It is used only for testing the model in a setting similar to the real-world, when it is deployed in production.\n",
        "2. Divide the design set into two separate sets:\n",
        "  - training set, used to train each of the candidate models\n",
        "  - validation set, used to score each of the trained candidate models\n",
        "3. Rank the models according to the performance metric (e.g., accuracy)\n",
        "4. Pick the model that performs the best on the validation set.\n",
        "5. Test the model on the testing set. This is an estimation on how the model will work in the real word.\n",
        "\n",
        "Note also that, as we will do later in the notebook, step 2 can be performed multiple times with separate splits, so that we can compute average performances across multiple experiments (and thus avoid \"lucky guesses\" of the parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMTkB1DBtefD"
      },
      "source": [
        "First, we will import some functions from the last notebook. We will use again scatter plots to represent the points in our dataset, and we will visualize the decision functions of the classifiers to evaluate visually how they behave with different hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LaZjBCWHrJ3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def get_cmap_and_colors(n_classes):\n",
        "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
        "    colors = prop_cycle.by_key()['color']\n",
        "    cmap = ListedColormap(colors[:n_classes])\n",
        "    return cmap, colors\n",
        "\n",
        "# taken from previous notebook\n",
        "def plot_dataset(x, y):\n",
        "    available_classes = np.unique(y)  # takes the unique elements\n",
        "    _, colors = get_cmap_and_colors(len(available_classes))\n",
        "    for c in sorted(available_classes):\n",
        "        samples_from_class = x[y==c]\n",
        "        x_coords = samples_from_class[:, 0]\n",
        "        y_coords = samples_from_class[:, 1]\n",
        "        plt.scatter(x_coords, y_coords, label=str(c), color=colors[c])  # plots also sets of points\n",
        "    plt.xlabel('feature x1')\n",
        "    plt.ylabel('feature x2')\n",
        "    plt.legend()\n",
        "\n",
        "# taken from previous notebook\n",
        "def plot_decision_regions(x, y, classifier, num_points=100):\n",
        "    cmap, _ = get_cmap_and_colors(len(np.unique(y)))\n",
        "    # get maximum and minimum value for each feature (with a bit of margin)\n",
        "    margin = 0.05\n",
        "    x1_min, x1_max = x[:, 0].min() - margin, x[:, 0].max() + margin\n",
        "    x2_min, x2_max = x[:, 1].min() - margin, x[:, 1].max() + margin\n",
        "\n",
        "\n",
        "    # create coordinate matrices from coordinate vectors\n",
        "    xx1, xx2 = np.meshgrid(\n",
        "        np.linspace(x1_min, x1_max, num_points),\n",
        "        np.linspace(x2_min, x2_max, num_points))\n",
        "\n",
        "    # get color value for each coordinate (as feature 1 and feature 2)\n",
        "    z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "\n",
        "    # shape again as a grid\n",
        "    z = z.reshape(xx1.shape)\n",
        "\n",
        "    # plot the colors\n",
        "    plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)\n",
        "\n",
        "    # set plot limits\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # plot class samples\n",
        "    plot_dataset(x,y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z39aWFSOPfC"
      },
      "source": [
        "We now sample fom the two-moon dataset, defined in scikit-learn. This dataset is more challenging than the one before, as the data are not easy to separate e.g., with a linear classifier. For this reason, we will use an SVM with a Gaussian kernel to classify these data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrAvk6LLOTPk"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# sample data\n",
        "x, y = ...\n",
        "plt.figure(figsize=(5,5))\n",
        "plot_dataset(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6xPvHO0Hr0Y"
      },
      "source": [
        "## Performance Evaluation\n",
        "\n",
        "As discussed above, we are risking overfitting if we use the testing data for finding good hyperparameters. This is because some information on the test set can \"leak\" into the model, making the experiment less realistic.\n",
        "\n",
        "We have to first hold out the training data and use it only at the end of the design pipeline.\n",
        "\n",
        "Then, from the remaining data, we can divide the dataset into training and validation sets. However, dividing many times the dataset might lead to small subsets. For this reason, we use k-fold cross validation, i.e., we split multiple times the design set and average the performances over multiple runs of the experiment.\n",
        "\n",
        "We can first take a look at what the classifier learns with ranges of hyperparameters. We will use a SVM classifier with a Gaussian kernel.\n",
        "\n",
        "The main parameters that we are going to change are `C` (the regularization parameter) and `gamma` (the kernel coefficient). We now define some values and test the classifiers with all the combinations of these two hyperparameters.\n",
        "\n",
        "First, let's define what happens in a single run. We have to create multiple splits, train a classifier on the training subset and test it on a validation set, and average the performances across the different splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7wb1a3s1-9i"
      },
      "outputs": [],
      "source": [
        "x, y = make_moons(n_samples=1000, noise=0.2)\n",
        "\n",
        "splitter = ...\n",
        "\n",
        "scaler = ...\n",
        "clf = ...\n",
        "clf_name = 'SVM RBF'\n",
        "\n",
        "for split_idx, (tr_idx, ts_idx) in enumerate(splitter.split(x, y)):\n",
        "    x_train = ...\n",
        "    y_train = ...\n",
        "    x_val = ...\n",
        "    y_val = ...\n",
        "\n",
        "    x_train = ...\n",
        "    x_val = ...\n",
        "\n",
        "    # train clf\n",
        "    ...\n",
        "    y_pred = ...\n",
        "\n",
        "    accuracy = ...\n",
        "\n",
        "    print(f\"Run: {split_idx} - Validation accuracy: {accuracy:.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6iz4P20kDon"
      },
      "outputs": [],
      "source": [
        "# plot the last classifier\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.title(clf_name)\n",
        "plot_decision_regions(x_val, y_val, clf)\n",
        "plt.text(0.05, 0.05, \"Val. Acc.: {:.1%}\".format(accuracy),\n",
        "        bbox=dict(facecolor='white'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx0bRkaN77J-"
      },
      "source": [
        "Now, let's code this procedure as a function `run`.\n",
        "We will store the accuracy values and then return the list of accuracies for each run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK17HIQ08CrR"
      },
      "outputs": [],
      "source": [
        "def run(x, y, splitter, scaler, clf):\n",
        "    \"\"\"Take input data (x,y), split it (n times), scale it,\n",
        "    learn classifier on training data, and evaluate the mean test error.\n",
        "    \"\"\"\n",
        "    acc = ...\n",
        "\n",
        "    for i, (tr_idx, ts_idx) in enumerate(splitter.split(x, y)):\n",
        "        xtr = x[tr_idx, :]\n",
        "        ytr = y[tr_idx]\n",
        "        xts = x[ts_idx, :]\n",
        "        yts = y[ts_idx]\n",
        "\n",
        "        xtr = scaler.fit_transform(xtr)\n",
        "        xts = scaler.transform(xts)\n",
        "\n",
        "        clf.fit(xtr, ytr)\n",
        "        ypred = clf.predict(xts)\n",
        "        acc[i] = ...\n",
        "    return acc\n",
        "\n",
        "\n",
        "# sample data\n",
        "x, y = make_moons(n_samples=500, noise=0.2)\n",
        "\n",
        "splitter = ShuffleSplit(n_splits=10, random_state=0, train_size=0.5)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "clf = svm.SVC(kernel='linear', C=1)\n",
        "acc = ...\n",
        "print(\"Mean test accuracy: {:.1%} +/- {:.1%}\".format(acc.mean(), 2*acc.std()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNz5rmu4iZsu"
      },
      "source": [
        "Let's test a few classifiers now and see how the decision function changes depending on the parameters.\n",
        "\n",
        "First, let's define the range of parameters (and let's get all the combinations).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv3awoUCGz_x"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "C_values = [1, 1e1, 1e2]\n",
        "gamma_values = [1e-1, 1, 1e2]\n",
        "\n",
        "combinations = ...\n",
        "\n",
        "print(combinations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edHcp1BjBNzx"
      },
      "source": [
        "Then, let's create a plot where we visualize the decision function for each couple of hyperparameters defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKNZBYrtHfsQ"
      },
      "outputs": [],
      "source": [
        "n_cols = len(C_values)\n",
        "n_rows = len(gamma_values)\n",
        "fig = plt.figure(figsize=(3*n_cols, 3*n_rows))\n",
        "for i, (C, gamma) in enumerate(combinations):\n",
        "    clf = ...\n",
        "    acc = ...\n",
        "    plt.subplot(n_rows, n_cols, i+1)\n",
        "    plot_decision_regions(x_val, y_val, clf)\n",
        "    plt.title(f\"C={C:.1e}, gamma={gamma:.1e}\")\n",
        "    plt.text(0.05, 0.05, \"Acc: {:.1%} +/- {:.1%}\".format(acc.mean(), 2*acc.std()),\n",
        "             bbox=dict(facecolor='white'))\n",
        "\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m16kJcYUBU-p"
      },
      "source": [
        "This already helps us finding a good-enough model for this simple problem. Of course, we might want to test more values, and we don't always have 2D data to visualize. For this reason, we will now build a loop to perform the sweep over the hyperparameters (grid search) and to find the best classifier depending on how it performs on the validation dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d655NO4wirFr"
      },
      "source": [
        "## Hyperparameter Estimation\n",
        "\n",
        "Often, in machine learning, we have to configure the algorithms depending on the data distribution and characteristics of the classification problem (e.g., number of classes, noise, etc.).\n",
        "\n",
        "To do so, we saw that we can perform grid search and cross validation to test the hyperparameters in multiple settings.\n",
        "\n",
        "Let's now rank the classifiers depending on their performances on the validation set(s).\n",
        "\n",
        "For each `C` value,  we have to compute the mean validation accuracy (averaged on the K folds). We have to do the same for each `gamma` value.\n",
        "Then, we will select the `C` and `gamma` value corresponding to the maximum mean validation accuracy, and use those values to train our classifier on the whole training set.\n",
        "\n",
        "This procedure is already implemented by the `GridSearchCV` wrapper class from sklearn, and we will use it along with our `run` function.\n",
        "\n",
        "Reference for grid search cross validation on sklearn:\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "\n",
        "Let's start by performing the search with the `make_moons` dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LR1CaqjBSNE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "x, y = make_moons(n_samples=1000, noise=0.2)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "splitter = ShuffleSplit(n_splits=5, random_state=0, train_size=0.5)\n",
        "\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "gamma_values = [1e-2, 1e-1, 1, 1e2, 1e3]\n",
        "\n",
        "# creates the grid search setup\n",
        "clf = ...\n",
        "\n",
        "acc = run(x, y, splitter, scaler, clf)\n",
        "\n",
        "print(\"Hyperparameter estimation (5-fold xval)\")\n",
        "print(\"    - Grid scores on development set:\")\n",
        "means = ...\n",
        "stds = ...\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "    print(\"        Acc: %0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
        "\n",
        "# test on some other data\n",
        "x_test, y_test = ...\n",
        "x_test = ...\n",
        "\n",
        "acc = run(x_test, y_test, splitter, scaler, clf)\n",
        "print(\"    - Best parameters set found on development set:\", clf.best_params_)\n",
        "print(f\"    - Accuracy of best clf on test set: {acc.mean():0.3f} (+/-{acc.std()*2:0.03f})\")\n",
        "\n",
        "plot_decision_regions(x_test, y_test, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7RwOLjviES5"
      },
      "source": [
        "## MNIST dataset\n",
        "\n",
        "We can now do the same on the MNIST data. These are more complicated data (784 features!), however we can still use an SVM to fit the data and obtain good performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXImL4dexRKK"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "def load_mnist(train=False, num_samples=None):\n",
        "    x, y = fetch_openml('mnist_784', version=1, cache=True, parser='auto', return_X_y=True)\n",
        "    x, y = x.to_numpy(), y.to_numpy()\n",
        "    if train:\n",
        "        # load mnist training (60,000 samples)\n",
        "        x, y = x[:60000, :], y[:60000]\n",
        "    else:\n",
        "        # load mnist test (10,000 samples)\n",
        "        x, y = x[60000:, :], y[60000:]\n",
        "\n",
        "    if num_samples is not None:\n",
        "        indexes = ...\n",
        "\n",
        "        # shuffle the indices\n",
        "        ...\n",
        "\n",
        "        num_samples = ...\n",
        "        x, y = ...\n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = load_mnist(train=True, num_samples=500)\n",
        "x_test, y_test = load_mnist(train=False, num_samples=500)\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LquqlIAHsjH"
      },
      "source": [
        "Let's visualize some of the samples (we have to elaborate the data to reconvert them to images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjvC7V8zxVnF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_digits(x, y, n=10):\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    for i in range(n**2):\n",
        "        plt.subplot(n,n,i+1)\n",
        "        # remember to reshape the samples to look like images!\n",
        "        plt.imshow(..., cmap='Greys')\n",
        "        plt.axis('off')\n",
        "\n",
        "plot_digits(x_train, y_train, n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWE9S1FT-3Gp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "splitter = ShuffleSplit(n_splits=5, random_state=0, train_size=0.5)\n",
        "\n",
        "C_values = np.logspace(-2, 3, 10)\n",
        "gamma_values = np.logspace(-3, 0, 10)\n",
        "\n",
        "# creates the grid search setup\n",
        "clf = GridSearchCV(estimator=svm.SVC(kernel='rbf'),\n",
        "                   param_grid={\"C\": C_values, \"gamma\": gamma_values})\n",
        "\n",
        "\n",
        "acc = run(x_train, y_train, splitter, scaler, clf)\n",
        "\n",
        "print(\"Hyperparameter estimation (5-fold xval)\")\n",
        "print(\"    - Grid scores on development set:\")\n",
        "means = clf.cv_results_['mean_test_score']\n",
        "stds = clf.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "    print(\"        Acc: %0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
        "\n",
        "\n",
        "acc = run(x_test, y_test, splitter, scaler, clf)\n",
        "print(\"    - Best parameters set found on development set:\", clf.best_params_)\n",
        "print(f\"    - Accuracy of best clf on test set: {acc.mean():0.3f} (+/-{acc.std()*2:0.03f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjPtt0dBH0TP"
      },
      "source": [
        "Then, we can also visualize the performances with a colormap. This helps us visually confirm that the best set of hyperparameters is indeed the one selected by the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex5ewpJnZI63"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.subplots_adjust(left=0.2, right=0.95, bottom=0.15, top=0.95)\n",
        "\n",
        "plt.imshow(\n",
        "    ...,\n",
        "    interpolation=\"nearest\",\n",
        "    cmap=plt.cm.seismic,\n",
        ")\n",
        "plt.xlabel(\"gamma\")\n",
        "plt.ylabel(\"C\")\n",
        "plt.colorbar()\n",
        "plt.xticks(np.arange(len(gamma_values)), map(lambda x: f\"{x:.2e}\", gamma_values), rotation=45)\n",
        "plt.yticks(np.arange(len(C_values)), map(lambda x: f\"{x:.2e}\", C_values))\n",
        "\n",
        "# puts values in the matrix\n",
        "for (j,i), label in np.ndenumerate(means.reshape(len(C_values), len(gamma_values))):\n",
        "    plt.text(i,j,f\"{label:.3f}\",ha='center',va='center', c=\"k\" if 0.4 < label < 0.6 else \"w\")\n",
        "\n",
        "plt.title(\"Validation accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notebook with complete code:\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/battistabiggio/ai4dev/blob/main/notebooks/AI4Dev_03_ml_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
