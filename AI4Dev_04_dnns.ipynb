{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/maurapintor/ai4dev/blob/main/AI4Dev_04_dnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to PyTorch\n",
        "\n",
        "In this notebook, we will look at PyTorch Tensors and introduce all the components used to implement deep learning one by one. \n",
        "Remember that the primary source of information, that also describes all the APIs and classes discussed in this notebook (and way more), is the [PyTorch documentation](https://pytorch.org/docs/stable/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can technically use numpy arrays for learning, but there are several aspects that they don't cover.\n",
        "\n",
        "In `PyTorch`, the `Tensor` class is more powerful than standard numeric libraries.\n",
        "\n",
        "* GPU support\n",
        "* Parallel operations on multiple devices or machines\n",
        "* Tensors keep track of graph of computations that created them (needed for gradients)\n",
        "\n",
        "All these features, especially the last one, are of utmost importance when dealing with deep learning!\n",
        "\n",
        "Mostly, the APIs for math operations resemble the ones of Numpy - but only from the exterior..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "x = torch.tensor([1., 2.])\n",
        "w = torch.tensor([2., 2.])\n",
        "\n",
        "# indexing operations as usual\n",
        "# the .item() extracts the element if it's only one number\n",
        "print(\"first element of w: \", w[0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... in the backstage, as specified, Tensors also keep track of the operations in the computational graph. This means that if we perform operations with tensors, we are able to retrieve all the computations from the end node to the beginning (leaves). This is needed to compute gradients automatically!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set tracking gradients to true, this will\n",
        "# rememeber all operations performed to w\n",
        "...\n",
        "\n",
        "# operations as usual\n",
        "...\n",
        "\n",
        "# compute gradients\n",
        "...\n",
        "\n",
        "# gradient of f w.r.t. w is x\n",
        "print(\"gradient of f w.r.t. w\")\n",
        "print(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional information on Tensor storage\n",
        "\n",
        "**Remember**: the underlying memory is allocated only once, which makes the view operation very lightweigth even for large storages. Even assigning another variable to the tensor only copies the reference!\n",
        "\n",
        "To retrieve the location of a tensor in memory, we can use the `data_ptr()` method:\n",
        "- https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html#torch-tensor-data-ptr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3, 4])\n",
        "b = a[0]  #Â different Tensor, same storage (points to the same location)\n",
        "c = a.reshape([2, 2])  # same storage, different stride\n",
        "\n",
        "print(\"storage of a == storage of c\")\n",
        "print(a.data_ptr() == c.data_ptr())  # same storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we modify the tensor c, we also indirectly modify the tensor a!\n",
        "\n",
        "To create a copy and duplicate the tensor, we can use the `Tensor.clone()` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c[0] = 1\n",
        "print(\"let's modify the first element of c\")\n",
        "\n",
        "print(\"as you can see, the tensor is the same, and c is only a reference\")\n",
        "print(\"first element of c\", c[0])\n",
        "print(\"first element of a\", a[0])\n",
        "\n",
        "print(\"storage of a == storage of c\")\n",
        "print(a.data_ptr() == c.data_ptr())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3, 4])\n",
        "d = a.clone()\n",
        "\n",
        "print(\"storage of a == storage of d\")\n",
        "print(a.data_ptr() == d.data_ptr())\n",
        "\n",
        "d[0] = 5\n",
        "print(\"first element of d\", d[0])\n",
        "print(\"first element of a\", a[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To reshape a Tensor, we have two APIs (mostly interchangeable for what we do here, but they have subtle differences related to the storage).\n",
        "\n",
        "To find out more:\n",
        "- https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape\n",
        "- https://discuss.pytorch.org/t/whats-the-difference-between-torch-reshape-vs-torch-view/159172"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3, 4, 5, 6])\n",
        "b = a.reshape((2, 3))\n",
        "c = a.view((3, 2))\n",
        "\n",
        "print(\"stride of a\")\n",
        "print(a.stride())  # how many storage items to skip for incrementing each dimension\n",
        "\n",
        "print(\"stride of b\")\n",
        "print(b.stride())  # how many storage items to skip for incrementing each dimension\n",
        "\n",
        "print(\"stride of c\")\n",
        "print(c.stride())  # how many storage items to skip for incrementing each dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning with tensors\n",
        "\n",
        "We can now use PyTorch to learn from tensors, given it's ability to track the operations. \n",
        "We will start with a simple **regression** problem, where we try to estimate a value given an underlying distribution of points.\n",
        "\n",
        "We do this by estimating the parameters of a line that passes through the points, thus once we have the value $x$, we can immediately compute $y=f(x) = w^T x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's create a dataset of points with scikit-learn. We will use one feature, that will be our `x`. We can wrap numpy arrays into tensors with the `tensor.from_numpy()` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq0wOQKSofjW",
        "outputId": "b48a256b-ea24-4792-86ac-554cb8d9f9c0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn import datasets\n",
        "\n",
        "samples, labels = datasets.make_regression(n_samples=1000, n_features=1, noise=2, random_state=42)\n",
        "\n",
        "samples, labels = torch.from_numpy(samples.ravel()), torch.from_numpy(labels)\n",
        "\n",
        "# normalization\n",
        "samples -= samples.min()\n",
        "samples /= samples.max()\n",
        "\n",
        "print(samples[:5], labels[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the points with `matplotlib.pyplot.scatter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "WBI6NrZBuXTS",
        "outputId": "5abe53ad-20d6-41da-d5de-34f231d0de05"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define a model that computes the $f(x)$. For now, let's stick to using python functions. Note that if the inputs of the function are tensors, all operations will also be tracked there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ody1OU8zujok"
      },
      "outputs": [],
      "source": [
        "def model(x, w, b):\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the model with the usual trick of defining a range of values for x and predicting with our model function the values for y. Then, we use `matplotlib.pyplot.plot` to draw the line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "TrUWlHm0vVpM",
        "outputId": "3a09c8fe-2b2f-4c4b-c297-e764d78c0df4"
      },
      "outputs": [],
      "source": [
        "def plot_line(w, b, alpha=1.0):\n",
        "    x_axis = torch.linspace(0, 1, 100)\n",
        "    y_axis = model(x_axis, w, b)\n",
        "    plt.plot(x_axis.detach().numpy(), y_axis.detach().numpy(), color='r', alpha=alpha)\n",
        "\n",
        "def plot_points(samples, labels):\n",
        "    plt.scatter(samples, labels)\n",
        "\n",
        "w, b = torch.tensor([1.0]), torch.tensor([0.0])\n",
        "\n",
        "plot_line(w, b)\n",
        "plot_points(samples, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we should write a loss function to perform gradient descent on the parameters (w and b) of our model.\n",
        "First, let's compute the gradients \"by hand\".\n",
        "\n",
        "$$\n",
        "\\displaystyle L(w, b, x) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - (w^T x_i + b)\\right)^2\n",
        "$$\n",
        "\n",
        "Then, the derivative of the loss w.r.t. $w$ is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial{w}} = \\frac{1}{n} \\sum_{i=1}^{n} 2 \\left(y_i - (w^T x_i + b)\\right) * x\n",
        "$$\n",
        "\n",
        "While the gradient of L w.r.t. b is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial{b}} = \\frac{1}{n} \\sum_{i=1}^{n} 2 \\left(y_i - (w^T x_i + b)\\right) * 1\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(y_pred, y_true):\n",
        "    ...\n",
        "\n",
        "def grad_l_w(x, w, b, y):\n",
        "    ...\n",
        "\n",
        "def grad_l_b(x, w, b, y):\n",
        "    ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w, b = torch.tensor([1.0]), torch.tensor([0.0])\n",
        "\n",
        "print(\"initial parameters\")\n",
        "print(w, b)\n",
        "\n",
        "n_steps = 2000\n",
        "step_size = 0.1\n",
        "\n",
        "# plot the points\n",
        "plot_points(samples, labels)\n",
        "\n",
        "for i in range(n_steps):\n",
        "    y_pred = model(samples, w, b)\n",
        "    loss = loss_fn(y_pred, labels)\n",
        "    if i % 100 == 0:\n",
        "        # print loss value and plot line with increasing alpha\n",
        "        print(f\"Iteration {i}, loss: {loss.item()}\")\n",
        "        plot_line(w, b, alpha=min(1.0, 0.2 + i/n_steps))\n",
        "    w = ...\n",
        "    b = ...\n",
        "\n",
        "# plot final line\n",
        "plot_line(w, b)\n",
        "\n",
        "print(\"final parameters\")\n",
        "print(w, b)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we don't want to compute the gradients by hand, we can also leverage the automatic gradient from PyTorch. The way we use it is:\n",
        "\n",
        "1. we compute the `forward` pass, that constructs the graph from the original tensors to the end of the graph (the loss node).\n",
        "2. we run the `backward`, that goes through the graph in reverse and accumulates the gradients\n",
        "\n",
        "The accumulation is a key concept. Since the graphs usually perform the computation for multiple samples at once, the default behavior of PyTorch is to accumulate all gradients in each node unless specified otherwise (by zero-ing operations).\n",
        "\n",
        "Let's compute the loss and print its value. We will notice an additional item in the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL-CpjGpunFY",
        "outputId": "14a8a7f2-af51-4462-b742-16b6150f04b3"
      },
      "outputs": [],
      "source": [
        "# now let's pack the parameters in one single tensor for convenience\n",
        "params = ...\n",
        "\n",
        "loss = loss_fn(model(samples, *params), labels)\n",
        "print(f\"loss: {loss}\")\n",
        "\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the printed tensor also contain a reference to a gradient function. This is the previous node of the graph. This is needed for the chain rule, in fact, remember how we compute the derivative of a composite function:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial{z}} = \\frac{\\partial{L}}{\\partial{u}} \\frac{\\partial{u}}{\\partial{z}}\n",
        "$$\n",
        "\n",
        "Thus, to compute the full gradient of the loss w.r.t. $w$, we have to compute the corresponding $\\frac{\\partial{u}}{\\partial{z}}$ of every operation and multiply by the first gradient encountered in the backward. Then, to get the next, we have to compute again the partial derivative of the next operation w.r.t. the previous node.\n",
        "\n",
        "This is all handled by PyTorch, thus we can directly access the gradient in the variable where we need it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybQDGh2wutc8",
        "outputId": "8f86c05b-2291-4fed-ee64-51105fdc65b1"
      },
      "outputs": [],
      "source": [
        "print(\"gradient of params\")\n",
        "print(params.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember also that the gradient stays there until we reset it. This means that if we run again a forward and a backward, we will fin 2*gradient in this variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss = loss_fn(model(samples, *params), labels)\n",
        "print(f\"loss: {loss}\")\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print(\"gradient accumulated after two backwards\")\n",
        "print(params.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to perform gradient descent, in each iteration we need to:\n",
        "\n",
        "1. compute the gradient\n",
        "2. update the parameters with the gradient\n",
        "3. clear the gradient from the node\n",
        "\n",
        "At the moment, we can use the inplace operation `tensor.zero_()` to set to zero the gradients after use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove the gradients for the next computation\n",
        "...\n",
        "\n",
        "print(\"gradients of w after zero-ing\")\n",
        "print(params.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm18giZcvS-5",
        "outputId": "f4a961a9-9c40-4c3c-94a2-d25da42f0345"
      },
      "outputs": [],
      "source": [
        "def training_loop(n_epochs, learning_rate, params, x, y):\n",
        "    plot_points(x, y)\n",
        "    for epoch in range(n_epochs):\n",
        "        y_pred = ...\n",
        "        loss = ...\n",
        "        # compute backward\n",
        "        ...\n",
        "        if epoch % 100 == 0:\n",
        "            print(\"Epoch: %d, Loss %f\" % (epoch, float(loss)))\n",
        "            plot_line(*params, alpha=min(1.0, 0.2 + epoch/n_epochs))\n",
        "        with torch.no_grad():  # this is required to avoid creating nested graphs\n",
        "            params = ...\n",
        "        # reset gradients\n",
        "        ...\n",
        "    plot_line(*params)\n",
        "    return params\n",
        "\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "print(\"initial parameters: \")\n",
        "print(params)\n",
        "trained_params = training_loop(2000, 1e-1, params, samples, labels)\n",
        "print(\"final parameters: \")\n",
        "print(trained_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can also use the optimizers from PyTorch to perform the update of the parameters.\n",
        "We have to pass to the constructor the set of parameters (even multiple ones) that we need to update. PyTorch will automatically track the gradients on these and use the gradients to update them, according to the \"rules\" of the optimizer defined. Let's for example use the Adam optimizer: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "mf_QBTexRhbB",
        "outputId": "f572b0aa-9b00-4c61-9a56-c090c5628879"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "def training_loop(n_epochs, learning_rate, params, x, y):\n",
        "    # define the optimizer\n",
        "    optimizer = ...\n",
        "    for epoch in range(n_epochs):\n",
        "        y_pred = model(x, *params)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        if epoch % 100 == 0:\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "            plot_line(*params, alpha=min(1.0, 0.2 + epoch/n_epochs))\n",
        "        # update the parameters\n",
        "        ...\n",
        "        # clears the gradients on the parameters\n",
        "        ...\n",
        "    return params\n",
        "\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "\n",
        "print(\"initial parameters\")\n",
        "print(params)\n",
        "training_loop(2000, 1e-1, params, samples, labels)\n",
        "\n",
        "plot_line(*params)\n",
        "plot_points(samples, labels)\n",
        "print(\"final parameters\")\n",
        "print(params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's introduce also a scheduler that will reduce the step size during training. We can update the step size by calling `scheduler.step()`. \n",
        "We will use now a `MultiStepLR`, which takes a set of milestones in which it will decay the step size by a factor `gamma`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "wrsK5QqoRoSD",
        "outputId": "abd92735-0540-463e-c154-bf7bf2f2ff39"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "\n",
        "def training_loop(n_epochs, learning_rate, params, x, y):\n",
        "    optimizer = SGD([params], lr=learning_rate)\n",
        "    scheduler = ...\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        y_pred = model(x, *params)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        if epoch % 100 == 0:\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "            plot_line(*params, alpha=min(1.0, 0.2 + epoch/n_epochs))\n",
        "        # update the parameters\n",
        "        optimizer.step()\n",
        "        # clears the gradients on the parameters\n",
        "        optimizer.zero_grad()\n",
        "        # updates the learning rate according to the scheduler rule\n",
        "        ...\n",
        "    return params\n",
        "\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "\n",
        "print(\"initial parameters\")\n",
        "print(params)\n",
        "# let's start from a bigger learning rate\n",
        "training_loop(2000, 1e-1, params, samples, labels)\n",
        "\n",
        "plot_line(*params)\n",
        "plot_points(samples, labels)\n",
        "print(\"final parameters\")\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now also replace the model function with a model from PyTorch. The simple linear model is implemented in the `torch.nn.Linear` class.\n",
        "\n",
        "(We have to modify the function to plot the model and the samples and labels slightly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "ef4cZzVpmSZJ",
        "outputId": "41bb4551-11bf-48b1-8748-53e8ccdecc8d"
      },
      "outputs": [],
      "source": [
        "def plot_module(model, alpha=1.0):\n",
        "    x_axis = torch.linspace(0, 1, 100).unsqueeze(1)\n",
        "    y_axis = model(x_axis)\n",
        "    plt.plot(x_axis.detach().numpy(), y_axis.detach().numpy(), color='r', alpha=alpha)\n",
        "\n",
        "def training_loop_module(n_epochs, learning_rate, model, x, y):  # changed params to model\n",
        "    x, y = x.float().unsqueeze(1), y.float().unsqueeze(1)\n",
        "    optimizer = Adam(...)  # changed [params] to model.parameters()\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[100, 500, 1000], gamma=0.5)\n",
        "    criterion = ...\n",
        "\n",
        "    plot_points(x, y)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        y_pred = model(x)  # changed line\n",
        "        loss = criterion(y_pred, y)\n",
        "        loss.backward()\n",
        "        if epoch % 100 == 0:\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "            plot_module(model, alpha=min(1.0, 0.2 + epoch/n_epochs))\n",
        "        # update the parameters\n",
        "        optimizer.step()\n",
        "        # clears the gradients on the parameters\n",
        "        optimizer.zero_grad()\n",
        "        # updates the learning rate according to the scheduler rule\n",
        "        scheduler.step()\n",
        "    \n",
        "    plot_module(model)\n",
        "\n",
        "    return params\n",
        "\n",
        "linear_model = torch.nn.Linear(1, 1)\n",
        "\n",
        "print(\"initial parameters\")\n",
        "print(linear_model.weight.item(), linear_model.bias.item())\n",
        "\n",
        "training_loop_module(2000, 1.0, linear_model, samples, labels)\n",
        "\n",
        "plot_module(linear_model)\n",
        "print(\"final parameters\")\n",
        "print(linear_model.weight.item(), linear_model.bias.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then, we can replace the linear layer with a composite (non-linear) PyTorch model. For now, let's use the `torch.nn.Sequential`, that allows us to list the sequence of layers and activations, that will be applied in cascade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "lA_LmLs2wQih",
        "outputId": "3b19e240-9c24-4a15-dcef-2613e7ee2be4"
      },
      "outputs": [],
      "source": [
        "dnn = torch.nn.Sequential(\n",
        "    ...\n",
        ")\n",
        "\n",
        "training_loop_module(2000, 1e-3, dnn, samples, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The advantage of having a non-linear model is that now we can also perform regression on non-linear distributions of samples.\n",
        "Let's modify the y so that the function mapping between x and y is not linear anymore, and let's visualize the new distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "ikHDnAy3A5RQ",
        "outputId": "a06d1068-ea81-4828-f93d-8e5d79575eb4"
      },
      "outputs": [],
      "source": [
        "# create modified ys\n",
        "y_new = ...\n",
        "...\n",
        "\n",
        "plot_points(samples, y_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "VOTGZCvUyvOS",
        "outputId": "ed9f007c-6015-4602-e8a4-0d2ba6f5869c"
      },
      "outputs": [],
      "source": [
        "y_new = labels.clone()\n",
        "y_new[samples>0.5] *= -2\n",
        "\n",
        "dnn = torch.nn.Sequential(\n",
        "    torch.nn.Linear(1, 100),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(100, 100),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(100, 1),\n",
        ")\n",
        "\n",
        "training_loop_module(1000, 1e-2, dnn, samples, y_new)\n",
        "\n",
        "plot_module(dnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's see how neural networks are implemented in PyTorch.\n",
        "We write a class that inherits the `torch.nn.Module`, that is the base of all differentiable modules in the library (including the losses). \n",
        "In the `__init__` we define the layers, and all parameters will be initialized automatically (also with better distributions than setting them manually). Additionally, all parameters initialized here will be accessible by using `model.parameters()` and inspected recursively. \n",
        "\n",
        "The other thing to implement here is the method `forward`, that defines all the operations performed on the input. When we want the output from our model, we call `model(x)` and this will call the forward method (along with other operations before and after, that are missed if you call `model.forward(x)`. As a rule, always use the `__call__` method)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "3OOdDOqK68PH",
        "outputId": "927100f1-4847-4229-91b6-7a068749cb8a"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Defines the layers\"\"\"\n",
        "        super().__init__()\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Defines the operations applied to x\"\"\"\n",
        "        ...\n",
        "\n",
        "dnn_2 = NeuralNetwork()\n",
        "training_loop_module(1000, 1e-2, dnn_2, samples, y_new)\n",
        "\n",
        "plot_module(dnn_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using GPUs\n",
        "\n",
        "We can move the computation to run on a Graphic Processing Unit (GPU) with a few simple lines. Note that if we run any operation on a GPU, all elements should be in the device. Thus, if we want to use our model in the GPU, we should take care of moving also the samples and labels. The rest of the code remains the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dnn_3 = NeuralNetwork()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dnn_3.to(device)\n",
        "samples, labels = samples.to(device), labels.to(device)\n",
        "\n",
        "training_loop_module(1000, 1e-2, dnn_3, samples, y_new)\n",
        "plot_module(dnn_3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving and loading tensors\n",
        "\n",
        "Until now, we created tensors only in RAM. At some point, we will want to store a tensor in the persistent memory. PyTorch uses `pickle` to **serialize** the tensors. Here is how to store a tensor in memory.\n",
        "\n",
        "```\n",
        "torch.save(a, 'tensor.pth')  # note that the extension is arbitrary\n",
        "```\n",
        "\n",
        "And to load back the tensor, a similar API is available.\n",
        "\n",
        "```\n",
        "b = torch.load('tensor.pth')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `torch.save` API uses `pickle` to save the Python object in memory. In theory, we could issue `torch.save(net)` and we can store the object somewhere in our memory. However, this has some issues. \n",
        "\n",
        "If we save the model directly, we risk problems when reloading the model (as we cannot save `torch.nn` inside a pickle). You can find the issue well described in the [PyTorch documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model). The correct way of saving the model is to save the model code in a `.py` file, and then use `torch.save` to store the parameters in a file. Here are the correct steps for saving and loading a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = 'cifar_model.pt'\n",
        "torch.save(dnn_3.state_dict(), model_path)\n",
        "\n",
        "new_model = NeuralNetwork()\n",
        "new_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "plot_points(samples, y_new)\n",
        "plot_module(dnn_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notebook with complete code:\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/battistabiggio/ai4dev/blob/main/notebooks/AI4Dev_04_dnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
